{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install facenet_pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ICbPuIx0xAnE","executionInfo":{"status":"ok","timestamp":1667503471341,"user_tz":-330,"elapsed":3519,"user":{"displayName":"Nisarg Patel","userId":"14948906446985767432"}},"outputId":"ed4d1dac-5ba9-4c8f-9708-7c59ba4f1612"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: facenet_pytorch in /usr/local/lib/python3.7/dist-packages (2.5.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (7.1.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (0.13.1+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (1.21.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch) (4.1.1)\n","Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch) (1.12.1+cu113)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PsRb5cHtz5N"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import sys\n","import dlib\n","import re\n","import torch\n","from torch import nn\n","# from model import AutoEncoder\n","from skimage import img_as_ubyte\n","from facenet_pytorch import MTCNN\n","import torch\n","import cv2\n","from PIL import Image\n","\n","import math"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnx7m-Ahzvwa","executionInfo":{"status":"ok","timestamp":1667503476092,"user_tz":-330,"elapsed":4758,"user":{"displayName":"Nisarg Patel","userId":"14948906446985767432"}},"outputId":"dd7f4e92-a169-48e1-a83a-a54fdde92552"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["detector = dlib.get_frontal_face_detector()\n","PREDICTOR_PATH = '/content/drive/MyDrive/Minor_Project/models/shape_predictor_68_face_landmarks.dat'\n","predictor = dlib.shape_predictor(PREDICTOR_PATH)"],"metadata":{"id":"bTI1OBvqt2Gk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rotate(image, output_size=256):\n","    image = image[:, :, ::-1]  # BGR to RGB\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","    # detect faces in the grayscale image\n","    rects = detector(gray, 1)\n","    if len(rects) > 0:\n","        # loop over the face detections\n","        for (i, rect) in enumerate(rects):\n","            shape = predictor(gray, rect)  # get facial features\n","            shape = np.array([(shape.part(j).x, shape.part(j).y) for j in range(shape.num_parts)])\n","\n","            # center and scale face around mid point between eyes\n","            center_eyes = shape[27].astype(int)\n","            eyes_d = np.linalg.norm(shape[36] - shape[45])\n","            face_size_x = int(eyes_d * 2.)\n","            if face_size_x < 50:\n","                continue\n","\n","            # rotate to normalized angle\n","            d = (shape[45] - shape[36]) / eyes_d  # normalized eyes-differnce vector (direction)\n","            a = np.rad2deg(np.arctan2(d[1], d[0]))  # angle\n","            scale_factor = float(output_size) / float(face_size_x * 2.)  # scale to fit in output_size\n","            # rotation (around center_eyes) + scale transform\n","            M = np.append(cv2.getRotationMatrix2D((int(center_eyes[0]),int(center_eyes[1])), a, scale_factor), [[0, 0, 1]], axis=0)\n","            # apply shift from center_eyes to middle of output_size\n","            M1 = np.array([[1., 0., -center_eyes[0] + output_size / 2.],\n","                           [0., 1., -center_eyes[1] + output_size / 2.],\n","                           [0, 0, 1.]])\n","            # concatenate transforms (rotation-scale + translation)\n","            M = M1.dot(M)[:2]\n","            # warp\n","            try:\n","                face = cv2.warpAffine(image, M, (output_size, output_size), borderMode=cv2.BORDER_REPLICATE)\n","            except:\n","                continue\n","            face = cv2.resize(face, (output_size, output_size), cv2.COLOR_BGR2RGB)\n","            return face"],"metadata":{"id":"SSIzfowatxdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Running on device: {}'.format(device))\n","mtcnn = MTCNN(keep_all=True, device=device, margin=50, select_largest=True, image_size=256)\n","\n","\n","def extract_face(frame, align=True, margin=5):\n","    if align:\n","        frame = rotate(np.array(frame))\n","    frame = Image.fromarray(frame)\n","    boxes, _ = mtcnn.detect(frame)\n","    for box in boxes:\n","        box_list = box.tolist()\n","        # bounding box coordinated\n","        x1 = int(box_list[0])\n","        y1 = int(box_list[1])\n","        x2 = int(box_list[2])\n","        y2 = int(box_list[3])\n","        #  find the middle of the image to get a perfect square, mtcnn gives a rectangle image of the face so making\n","        #  the image a square makes it easier to train\n","        y1 += margin\n","        y2 -= margin\n","        diff = abs(y1 - y2)\n","        mid_x = (x2 + x1) // 2\n","        # mid_y = (y2 + y1) // 2\n","        x1 = mid_x - (diff // 2)\n","        x2 = mid_x + (diff // 2)\n","        return frame.crop((x1, y1, x2, y2))  # sends back only the square around the face, possible no face detected\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sL3x27qiwhJp","executionInfo":{"status":"ok","timestamp":1667503476093,"user_tz":-330,"elapsed":12,"user":{"displayName":"Nisarg Patel","userId":"14948906446985767432"}},"outputId":"1ec00d38-02ae-48bd-ae75-0d12d868c2d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on device: cuda:0\n"]}]},{"cell_type":"code","source":[" # Read points from text file\n","def readPoints(path):\n","    # Create an array of points.\n","    points = []\n","\n","    # Read points\n","    with open(path) as file:\n","        for line in file:\n","            x, y = line.split()\n","            points.append((int(x), int(y)))\n","\n","    return points\n","\n","\n","# Apply affine transform calculated using srcTri and dstTri to src and\n","# output an image of size.\n","def applyAffineTransform(src, srcTri, dstTri, size):\n","    # Given a pair of triangles, find the affine transform.\n","    warpMat = cv2.getAffineTransform(np.float32(srcTri), np.float32(dstTri))\n","\n","    # Apply the Affine Transform just found to the src image\n","    dst = cv2.warpAffine(src, warpMat, (size[0], size[1]), None, flags=cv2.INTER_LINEAR,\n","                         borderMode=cv2.BORDER_REFLECT_101)\n","\n","    return dst\n","\n","\n","# Check if a point is inside a rectangle\n","def rectContains(rect, point):\n","    if point[0] < rect[0]:\n","        return False\n","    elif point[1] < rect[1]:\n","        return False\n","    elif point[0] > rect[0] + rect[2]:\n","        return False\n","    elif point[1] > rect[1] + rect[3]:\n","        return False\n","    return True\n","\n","\n","# calculate delanauy triangle\n","def calculateDelaunayTriangles(rect, points):\n","    # create subdiv\n","    subdiv = cv2.Subdiv2D(rect)\n","    # Insert points into subdiv\n","    for p in points:\n","        p = tuple(p)\n","        subdiv.insert(p)\n","\n","    triangleList = subdiv.getTriangleList()\n","\n","    delaunayTri = []\n","\n","    pt = []\n","\n","    for t in triangleList:\n","        pt.append((t[0], t[1]))\n","        pt.append((t[2], t[3]))\n","        pt.append((t[4], t[5]))\n","\n","        pt1 = (t[0], t[1])\n","        pt2 = (t[2], t[3])\n","        pt3 = (t[4], t[5])\n","\n","        if rectContains(rect, pt1) and rectContains(rect, pt2) and rectContains(rect, pt3):\n","            ind = []\n","            # Get face-points (from 68 face detector) by coordinates\n","            for j in range(0, 3):\n","                for k in range(0, len(points)):\n","                    if (abs(pt[j][0] - points[k][0]) < 1.0 and abs(pt[j][1] - points[k][1]) < 1.0):\n","                        ind.append(k)\n","                        # Three points form a triangle. Triangle array corresponds to the file tri.txt in FaceMorph\n","            if len(ind) == 3:\n","                delaunayTri.append((ind[0], ind[1], ind[2]))\n","\n","        pt = []\n","\n","    return delaunayTri\n","\n","\n","# Warps and alpha blends triangular regions from img1 and img2 to img\n","def warpTriangle(img1, img2, t1, t2):\n","    # Find bounding rectangle for each triangle\n","    r1 = cv2.boundingRect(np.float32([t1]))\n","    r2 = cv2.boundingRect(np.float32([t2]))\n","    # Offset points by left top corner of the respective rectangles\n","    t1Rect = []\n","    t2Rect = []\n","    t2RectInt = []\n","\n","    for i in range(0, 3):\n","        t1Rect.append(((t1[i][0] - r1[0]), (t1[i][1] - r1[1])))\n","        t2Rect.append(((t2[i][0] - r2[0]), (t2[i][1] - r2[1])))\n","        t2RectInt.append((int(t2[i][0] - r2[0]),int(t2[i][1] - r2[1])))\n","\n","    # Get mask by filling triangle\n","    mask = np.zeros((r2[3], r2[2], 3), dtype=np.float32)\n","    cv2.fillConvexPoly(mask, np.int32(t2RectInt) , (1.0, 1.0, 1.0), 16, 0)\n","    # Apply warpImage to small rectangular patches\n","    img1Rect = img1[r1[1]:r1[1] + r1[3], r1[0]:r1[0] + r1[2]]\n","    # img2Rect = np.zeros((r2[3], r2[2]), dtype = img1Rect.dtype)\n","\n","    size = (r2[2], r2[3])\n","\n","    img2Rect = applyAffineTransform(img1Rect, t1Rect, t2Rect, size)\n","\n","    img2Rect = img2Rect * mask\n","\n","    # Copy triangular region of the rectangular patch to the output image\n","    img2[r2[1]:r2[1] + r2[3], r2[0]:r2[0] + r2[2]] = img2[r2[1]:r2[1] + r2[3], r2[0]:r2[0] + r2[2]] * (\n","                (1.0, 1.0, 1.0) - mask)\n","\n","    img2[r2[1]:r2[1] + r2[3], r2[0]:r2[0] + r2[2]] = img2[r2[1]:r2[1] + r2[3], r2[0]:r2[0] + r2[2]] + img2Rect\n","\n","\n","def get_landmarks(im):\n","    rects = detector(im, 1)\n","\n","    if len(rects) > 1:\n","        # print(\"Too Many Faces\") #raise TooManyFaces\n","        return []\n","    if len(rects) == 0:\n","        # print(\"No Faces\") #raise NoFaces\n","        return []\n","\n","    return np.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n","\n","\n","def atof(text):\n","    try:\n","        retval = float(text)\n","    except ValueError:\n","        retval = text\n","    return retval\n","\n","\n","def natural_keys(text):\n","    return [atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text)]\n","\n","\n","def swap_faces(img1, img2):\n","    img1Warped = np.copy(img2)\n","\n","    points1 = get_landmarks(img1)\n","    points1 = points1.tolist()\n","\n","    points2 = get_landmarks(img2)\n","    points2 = points2.tolist()\n","\n","    # Find convex hull\n","    hull1 = []\n","    hull2 = []\n","    hullIndex = cv2.convexHull(np.array(points2), returnPoints=False)\n","    for i in range(0, len(hullIndex)):\n","        hull1.append(points1[int(hullIndex[i])])\n","        hull2.append(points2[int(hullIndex[i])])\n","\n","    # Find delanauy traingulation for convex hull points\n","    sizeImg2 = img2.shape\n","    rect = (0, 0, sizeImg2[1], sizeImg2[0])\n","    dt = calculateDelaunayTriangles(rect, hull2)\n","    if len(dt) == 0:\n","        quit()\n","    # Apply affine transformation to Delaunay triangles\n","    for i in range(0, len(dt)):\n","        t1 = []\n","        t2 = []\n","\n","        # get points for img1, img2 corresponding to the triangles\n","        for j in range(0, 3):\n","            t1.append(hull1[dt[i][j]])\n","            t2.append(hull2[dt[i][j]])\n","\n","        warpTriangle(img1, img1Warped, t1, t2) # tracked error upto here\n","\n","    # Calculate Mask\n","    hull8U = []\n","    for i in range(0, len(hull2)):\n","        hull8U.append((hull2[i][0], hull2[i][1]))\n","\n","    mask = np.zeros(img2.shape, dtype=img2.dtype)\n","\n","    cv2.fillConvexPoly(mask, np.int32(hull8U), (255, 255, 255))\n","\n","    r = cv2.boundingRect(np.float32([hull2]))\n","\n","    center = ( (int(r[0]) + int(r[2] / 2)), (int(r[1]) + int(r[3] / 2)) )\n","    # Clone seamlessly\n","\n","    return cv2.seamlessClone(np.uint8(img1Warped), img2, mask, center, cv2.NORMAL_CLONE)"],"metadata":{"id":"6SAwKfhevd34"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_2yatG4ozLF"},"outputs":[],"source":["class Flatten(nn.Module):\n","    def forward(self, inputs):\n","        return inputs.view(inputs.size(0), -1)\n","\n","\n","class UnFlatten(nn.Module):\n","    def forward(self, inputs, size=512):\n","        return inputs.view(inputs.size(0), 128, 4, 4)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 128, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 32, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(32, 16, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(16, 4, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, img):\n","        validity = self.model(img)\n","        return (validity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XruQE2EhpPoV"},"outputs":[],"source":["class ResBlock(nn.Module):\n","    def __init__(self, n_ch) -> None:\n","        super().__init__()\n","\n","        self.resblock_model = nn.Sequential(\n","            nn.Conv2d(n_ch, n_ch, kernel_size=3, bias=False, padding=1),\n","            nn.BatchNorm2d(n_ch),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Conv2d(n_ch, n_ch, kernel_size=3, bias=False, padding=1),\n","            nn.BatchNorm2d(n_ch)\n","        )\n","\n","    def forward(self, inputs):\n","        return self.resblock_model(inputs) + inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnj1N2GQpQM_"},"outputs":[],"source":["class Downscale(nn.Module):\n","    def __init__(self, in_ch, out_ch, kernel_size=3, padding=1):\n","        super().__init__()\n","        self.in_ch = in_ch\n","        self.out_ch = out_ch\n","        self.kernel_size = kernel_size\n","        self.conv = nn.Conv2d(self.in_ch, self.out_ch, kernel_size=self.kernel_size, stride=2, padding=padding)\n","        self.batch_norm = nn.BatchNorm2d(self.out_ch)\n","        self.relu = nn.LeakyReLU(0.1)\n","        self.drop = nn.Dropout2d()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.batch_norm(x)\n","        x = self.relu(x)\n","        x = self.drop(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7O8QDxFpVC-"},"outputs":[],"source":["class Upscale(nn.Module):\n","    def __init__(self, in_ch, out_ch, kernel_size=5, padding=2):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose2d(in_ch, out_ch, kernel_size, stride=2, padding=1)\n","        self.batch_norm = nn.BatchNorm2d(out_ch)\n","        self.relu = nn.LeakyReLU(0.1)\n","        self.drop = nn.Dropout2d()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.batch_norm(x)\n","        x = self.relu(x)\n","        x = self.drop(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-AIfcrjh7IE"},"outputs":[],"source":["class AutoEncoder(nn.Module):\n","\n","    def __init__(self, image_channels=3, h_dim=2048, z_dim=128):\n","        super(AutoEncoder, self).__init__()\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.encoder = nn.Sequential(\n","            Downscale(image_channels, 64),\n","            Downscale(64, 128),\n","            # Downscale(128, 128),\n","            # ResBlock(128),\n","            Downscale(128, 256),\n","            # ResBlock(256),\n","            Downscale(256, 256),\n","            Downscale(256, 512),\n","            # ResBlock(512),\n","            Downscale(512, 512),\n","            Flatten(),\n","        )\n","        # ([32, 2304])\n","\n","        self.inter_layer = nn.Sequential(\n","            nn.Linear(h_dim, z_dim),\n","            nn.Linear(z_dim, z_dim),\n","            nn.Linear(z_dim, h_dim),\n","        )\n","\n","        self.decoder = nn.Sequential(\n","            UnFlatten(),\n","            # Upscale(128, 128, kernel_size=4),\n","            Upscale(128, 256, kernel_size=4),\n","            # ResBlock(256),\n","            # ResBlock(128),\n","            # ResBlock(128),\n","            Upscale(256, 256, kernel_size=4),\n","            Upscale(256, 128, kernel_size=4),\n","            # ResBlock(128),\n","            Upscale(128, 64, kernel_size=4),\n","            ResBlock(64),\n","            Upscale(64, 32, kernel_size=4),\n","            Upscale(32, 32, kernel_size=4),\n","            nn.Conv2d(32, image_channels, kernel_size=1, stride=2),\n","            nn.Sigmoid(),\n","        )\n","\n","        self.decoder_b = nn.Sequential(\n","            UnFlatten(),\n","            # Upscale(128, 128, kernel_size=4),\n","            Upscale(128, 256, kernel_size=4),\n","            # ResBlock(256),\n","            # ResBlock(128),\n","            # ResBlock(128),\n","            Upscale(256, 256, kernel_size=4),\n","            Upscale(256, 128, kernel_size=4),\n","            # ResBlock(128),\n","            Upscale(128, 64, kernel_size=4),\n","            ResBlock(64),\n","            Upscale(64, 32, kernel_size=4),\n","            Upscale(32, 32, kernel_size=4),\n","            nn.Conv2d(32, image_channels, kernel_size=1, stride=2),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x, version='a'):\n","        z = self.encoder(x)\n","        z = self.inter_layer(z)\n","        if version == 'a':\n","            z = self.decoder(z)\n","        else:\n","            z = self.decoder_b(z)\n","        return z"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/Minor_Project/Dataset/videos/elon.mp4\")\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","\n","model = AutoEncoder(image_channels=3).to(device)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Minor_Project/saved_models/final.pth\"))\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","video_tracked = cv2.VideoWriter('{}.mp4'.format(\"Swapped_Video\"), fourcc, fps, (width, height))\n","i = 0\n","decoder = \"b\"\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if ret:\n","        try:\n","            print('\\rTracking frame: {}'.format(i + 1), end='')\n","            i += 1\n","            # Retrive face from frame, align it, resize it in cv2 to fit into model\n","            img1_face = extract_face(frame)\n","            img1_face = np.array(img1_face)\n","            img1_face = cv2.resize(img1_face, (128, 128))\n","\n","            #  convert the frame\n","            frame = np.array(frame)\n","\n","            #  pytorch takes in channel, height and width,  so transpose to change into correct dimensions\n","            img1_face = cv2.cvtColor(img1_face, cv2.COLOR_BGR2RGB)\n","            img_tensor = img1_face[:, :, ::-1].transpose((2, 0, 1)).copy()  # chw, RGB order,[0,255]\n","            img_tensor = torch.from_numpy(img_tensor).float().div(255)  # chw , FloatTensor type,[0,1]\n","            img_tensor = img_tensor.unsqueeze(0)  # nch*w\n","            x = img_tensor.to(device)\n","            model.eval()\n","            out = model(x, version=\"b\")\n","            # convert the pytorch output into cv2\n","            out = out.data.cpu().squeeze().numpy()\n","            out = np.transpose(out, (1, 2, 0))\n","            out = cv2.cvtColor(out, cv2.COLOR_RGB2BGR)\n","            out = img_as_ubyte(out)\n","            out2 = swap_faces(out, frame)\n","            video_tracked.write(out2)\n","\n","        except Exception as e:\n","            print(e)\n","    else:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuY6X22Yt7PS","executionInfo":{"status":"ok","timestamp":1667503611283,"user_tz":-330,"elapsed":135197,"user":{"displayName":"Nisarg Patel","userId":"14948906446985767432"}},"outputId":"892f1e08-aa51-4f04-e7f9-b80bce98a3b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tracking frame: 11OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 16OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 19OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 20OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 42OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 43OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 51OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 52OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 54OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 59OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 89OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n","\n","Tracking frame: 90"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Qyct1hSg5q2j"},"execution_count":null,"outputs":[]}]}