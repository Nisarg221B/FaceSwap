{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"qDplYgxMZ7LS","executionInfo":{"status":"ok","timestamp":1667623080607,"user_tz":-330,"elapsed":4300,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torchvision.transforms import ToTensor\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","from torchvision.utils import save_image\n","import numpy as np\n","import cv2  \n","import random"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ye6XRHfBc4q1","outputId":"09b59982-a561-4da4-ecb0-c8d6457cf3c7","executionInfo":{"status":"ok","timestamp":1667623145647,"user_tz":-330,"elapsed":65045,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"b_2yatG4ozLF","executionInfo":{"status":"ok","timestamp":1667623145648,"user_tz":-330,"elapsed":13,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["class Flatten(nn.Module):\n","    def forward(self, inputs):\n","        return inputs.view(inputs.size(0), -1)\n","\n","\n","class UnFlatten(nn.Module):\n","    def forward(self, inputs, size=512):\n","        return inputs.view(inputs.size(0), 128, 4, 4)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 128, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 32, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(32, 16, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(16, 4, kernel_size=5, stride=2, padding=2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, img):\n","        validity = self.model(img)\n","        return (validity)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"XruQE2EhpPoV","executionInfo":{"status":"ok","timestamp":1667623145648,"user_tz":-330,"elapsed":10,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["class ResBlock(nn.Module):\n","    def __init__(self, n_ch) -> None:\n","        super().__init__()\n","\n","        self.resblock_model = nn.Sequential(\n","            nn.Conv2d(n_ch, n_ch, kernel_size=3, bias=False, padding=1),\n","            nn.BatchNorm2d(n_ch),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Conv2d(n_ch, n_ch, kernel_size=3, bias=False, padding=1),\n","            nn.BatchNorm2d(n_ch)\n","        )\n","\n","    def forward(self, inputs):\n","        return self.resblock_model(inputs) + inputs"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"fnj1N2GQpQM_","executionInfo":{"status":"ok","timestamp":1667623145649,"user_tz":-330,"elapsed":11,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["class Downscale(nn.Module):\n","    def __init__(self, in_ch, out_ch, kernel_size=3, padding=1):\n","        super().__init__()\n","        self.in_ch = in_ch\n","        self.out_ch = out_ch\n","        self.kernel_size = kernel_size\n","        self.conv = nn.Conv2d(self.in_ch, self.out_ch, kernel_size=self.kernel_size, stride=2, padding=padding)\n","        self.batch_norm = nn.BatchNorm2d(self.out_ch)\n","        self.relu = nn.LeakyReLU(0.1)\n","        self.drop = nn.Dropout2d()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.batch_norm(x)\n","        x = self.relu(x)\n","        x = self.drop(x)\n","        return x"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"M7O8QDxFpVC-","executionInfo":{"status":"ok","timestamp":1667623145649,"user_tz":-330,"elapsed":10,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["class Upscale(nn.Module):\n","    def __init__(self, in_ch, out_ch, kernel_size=5, padding=2):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose2d(in_ch, out_ch, kernel_size, stride=2, padding=1)\n","        self.batch_norm = nn.BatchNorm2d(out_ch)\n","        self.relu = nn.LeakyReLU(0.1)\n","        self.drop = nn.Dropout2d()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.batch_norm(x)\n","        x = self.relu(x)\n","        x = self.drop(x)\n","        return x"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"j-AIfcrjh7IE","executionInfo":{"status":"ok","timestamp":1667623145650,"user_tz":-330,"elapsed":10,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["class AutoEncoder(nn.Module):\n","\n","    def __init__(self, image_channels=3, h_dim=2048, z_dim=128):\n","        super(AutoEncoder, self).__init__()\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.encoder = nn.Sequential(\n","            Downscale(image_channels, 64),\n","            Downscale(64, 128),\n","            # Downscale(128, 128),\n","            # ResBlock(128),\n","            Downscale(128, 256),\n","            # ResBlock(256),\n","            Downscale(256, 256),\n","            Downscale(256, 512),\n","            # ResBlock(512),\n","            Downscale(512, 512),\n","            Flatten(),\n","        )\n","        # ([32, 2304])\n","\n","        self.inter_layer = nn.Sequential(\n","            nn.Linear(h_dim, z_dim),\n","            nn.Linear(z_dim, z_dim),\n","            nn.Linear(z_dim, h_dim),\n","        )\n","\n","        self.decoder = nn.Sequential(\n","            UnFlatten(),\n","            # Upscale(128, 128, kernel_size=4),\n","            Upscale(128, 256, kernel_size=4),\n","            # ResBlock(256),\n","            # ResBlock(128),\n","            # ResBlock(128),\n","            Upscale(256, 256, kernel_size=4),\n","            Upscale(256, 128, kernel_size=4),\n","            # ResBlock(128),\n","            Upscale(128, 64, kernel_size=4),\n","            ResBlock(64),\n","            Upscale(64, 32, kernel_size=4),\n","            Upscale(32, 32, kernel_size=4),\n","            nn.Conv2d(32, image_channels, kernel_size=1, stride=2),\n","            nn.Sigmoid(),\n","        )\n","\n","        self.decoder_b = nn.Sequential(\n","            UnFlatten(),\n","            # Upscale(128, 128, kernel_size=4),\n","            Upscale(128, 256, kernel_size=4),\n","            # ResBlock(256),\n","            # ResBlock(128),\n","            # ResBlock(128),\n","            Upscale(256, 256, kernel_size=4),\n","            Upscale(256, 128, kernel_size=4),\n","            # ResBlock(128),\n","            Upscale(128, 64, kernel_size=4),\n","            ResBlock(64),\n","            Upscale(64, 32, kernel_size=4),\n","            Upscale(32, 32, kernel_size=4),\n","            nn.Conv2d(32, image_channels, kernel_size=1, stride=2),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x, version='a'):\n","        z = self.encoder(x)\n","        z = self.inter_layer(z)\n","        if version == 'a':\n","            z = self.decoder(z)\n","        else:\n","            z = self.decoder_b(z)\n","        return z"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"XpxGHxf-w6Wn","executionInfo":{"status":"ok","timestamp":1667623145650,"user_tz":-330,"elapsed":10,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["import os\n","from math import exp\n","import torch.nn.functional as F\n","from torch.autograd import Variable"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ohFqAJcHQwLV","executionInfo":{"status":"ok","timestamp":1667623145651,"user_tz":-330,"elapsed":10,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["class Iterator:\n","    def __init__(self, dataset, batch_size=32):\n","        self.datset = dataset\n","        self.max = len(dataset)\n","        self.batch_size = batch_size\n","        self.idx = 0\n","\n","    def __iter__(self):\n","        self.idx = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.idx + + self.batch_size >= self.max - 1:\n","            np.random.shuffle(self.datset)\n","            self.idx = 0\n","        self.idx += self.batch_size\n","        return self.datset[self.idx:self.idx + self.batch_size]"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Ri8yKu1bzUbj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"974dcec8-5248-4521-e498-d00385581945","executionInfo":{"status":"ok","timestamp":1667623152289,"user_tz":-330,"elapsed":6648,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":10}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = AutoEncoder(image_channels=3).to(device)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Minor_Project/saved_models/final.pth'))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"OeXZRChHNmxZ","executionInfo":{"status":"ok","timestamp":1667623152290,"user_tz":-330,"elapsed":8,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ad4NrOMx4egO","executionInfo":{"status":"ok","timestamp":1667623152290,"user_tz":-330,"elapsed":6,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[],"source":["import os\n","from PIL import Image"]},{"cell_type":"code","source":["!pip install facenet-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8nYd4c6-Gsm","outputId":"bf371a75-750e-476e-d5e2-bf017a9cdcb1","executionInfo":{"status":"ok","timestamp":1667623156741,"user_tz":-330,"elapsed":4457,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 33.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.21.6)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.13.1+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2022.9.24)\n","Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (4.1.1)\n","Installing collected packages: facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.2\n"]}]},{"cell_type":"code","source":["from skimage import img_as_ubyte\n","from facenet_pytorch import MTCNN"],"metadata":{"id":"YM_Rye2oFt1H","executionInfo":{"status":"ok","timestamp":1667623223143,"user_tz":-330,"elapsed":599,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Running on device: {}'.format(device))\n","mtcnn = MTCNN(keep_all=True, device=device, margin=50, select_largest=True, image_size=256)\n","\n","\n","def extract_face(frame, align=True, margin=5):\n","    if align:\n","        frame = rotate(np.array(frame))\n","    frame = Image.fromarray(frame)\n","    boxes, _ = mtcnn.detect(frame)\n","    for box in boxes:\n","        box_list = box.tolist()\n","        # bounding box coordinated\n","        x1 = int(box_list[0])\n","        y1 = int(box_list[1])\n","        x2 = int(box_list[2])\n","        y2 = int(box_list[3])\n","        #  find the middle of the image to get a perfect square, mtcnn gives a rectangle image of the face so making\n","        #  the image a square makes it easier to train\n","        y1 += margin\n","        y2 -= margin\n","        diff = abs(y1 - y2)\n","        mid_x = (x2 + x1) // 2\n","        # mid_y = (y2 + y1) // 2\n","        x1 = mid_x - (diff // 2)\n","        x2 = mid_x + (diff // 2)\n","        return frame.crop((x1, y1, x2, y2))  # sends back only the square around the face, possible no face detected\n","\n","import dlib\n","detector = dlib.get_frontal_face_detector()\n","PREDICTOR_PATH = '/content/drive/MyDrive/Minor_Project/models/shape_predictor_68_face_landmarks.dat'\n","predictor = dlib.shape_predictor(PREDICTOR_PATH)\n","\n","def rotate(image, output_size=256):\n","    image = image[:, :, ::-1]  # BGR to RGB\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","    # detect faces in the grayscale image\n","    rects = detector(gray, 1)\n","    if len(rects) > 0:\n","        # loop over the face detections\n","        for (i, rect) in enumerate(rects):\n","            shape = predictor(gray, rect)  # get facial features\n","            shape = np.array([(shape.part(j).x, shape.part(j).y) for j in range(shape.num_parts)])\n","\n","            # center and scale face around mid point between eyes\n","            center_eyes = shape[27].astype(int)\n","            eyes_d = np.linalg.norm(shape[36] - shape[45])\n","            face_size_x = int(eyes_d * 2.)\n","            if face_size_x < 50:\n","                continue\n","\n","            # rotate to normalized angle\n","            d = (shape[45] - shape[36]) / eyes_d  # normalized eyes-differnce vector (direction)\n","            a = np.rad2deg(np.arctan2(d[1], d[0]))  # angle\n","            scale_factor = float(output_size) / float(face_size_x * 2.)  # scale to fit in output_size\n","            # rotation (around center_eyes) + scale transform\n","            M = np.append(cv2.getRotationMatrix2D((int(center_eyes[0]),int(center_eyes[1])), a, scale_factor), [[0, 0, 1]], axis=0)\n","            # apply shift from center_eyes to middle of output_size\n","            M1 = np.array([[1., 0., -center_eyes[0] + output_size / 2.],\n","                           [0., 1., -center_eyes[1] + output_size / 2.],\n","                           [0, 0, 1.]])\n","            # concatenate transforms (rotation-scale + translation)\n","            M = M1.dot(M)[:2]\n","            # warp\n","            try:\n","                face = cv2.warpAffine(image, M, (output_size, output_size), borderMode=cv2.BORDER_REPLICATE)\n","            except:\n","                continue\n","            face = cv2.resize(face, (output_size, output_size), cv2.COLOR_BGR2RGB)\n","            return face"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bj9Ar5VPA13x","outputId":"7f3580a6-1a86-4dd7-8a34-9f46ef0dd3a1","executionInfo":{"status":"ok","timestamp":1667623226410,"user_tz":-330,"elapsed":2662,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on device: cuda:0\n"]}]},{"cell_type":"code","execution_count":17,"metadata":{"id":"vt_oGvDYzvta","colab":{"base_uri":"https://localhost:8080/"},"outputId":"60fc2ffe-504d-4c81-9c96-7d1770a54fee","executionInfo":{"status":"ok","timestamp":1667623278631,"user_tz":-330,"elapsed":52224,"user":{"displayName":"NISARG PATEL","userId":"08923139073482849821"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tracking frame: 90"]}],"source":["def transfer(model, x, version):\n","    x = torch.from_numpy(x).unsqueeze(0)\n","    x = x.to('cuda')\n","    model.eval()\n","    if version == 'a':\n","        out = model(x, version='a')\n","        return torch.cat([x, out])\n","    elif version == 'b':\n","        out = model(x, version='b')\n","        return torch.cat([x, out])\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/Minor_Project/Dataset/videos/elon.mp4\")\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","\n","model = AutoEncoder(image_channels=3).to(device)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Minor_Project/saved_models/final.pth\"))\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","video_tracked = cv2.VideoWriter('{}.mp4'.format(\"SwappedFaceVideo\"), fourcc, fps, (width, height))\n","i = 0\n","\n","def write_images(model, image_a, dir_name):\n","    if not os.path.exists(dir_name):\n","        os.makedirs(dir_name)\n","\n","    out = transfer(model, image_a, 'b')\n","    # convert the pytorch output into cv2\n","    out = out.data.cpu().squeeze().numpy()\n","    out = np.transpose(out, (1, 2, 0))\n","    out = cv2.cvtColor(out, cv2.COLOR_RGB2BGR)\n","    out = img_as_ubyte(out)\n","    video_tracked.write(out)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/Minor_Project/Dataset/videos/elon.mp4\")\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","\n","model = AutoEncoder(image_channels=3).to(device)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Minor_Project/saved_models/final.pth\"))\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","video_tracked = cv2.VideoWriter('{}.mp4'.format(\"Swapped_Video\"), fourcc, fps, (width, height))\n","i = 0\n","decoder = \"b\"\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if ret:\n","        try:\n","            print('\\rTracking frame: {}'.format(i + 1), end='')\n","            i += 1\n","            # Retrive face from frame, align it, resize it in cv2 to fit into model\n","            img1_face = extract_face(frame)\n","            img1_face = np.array(img1_face)\n","            img1_face = cv2.resize(img1_face, (128, 128))\n","\n","            #  convert the frame\n","            frame = np.array(frame)\n","\n","            #  pytorch takes in channel, height and width,  so transpose to change into correct dimensions\n","            img1_face = cv2.cvtColor(img1_face, cv2.COLOR_BGR2RGB)\n","            img_tensor = img1_face[:, :, ::-1].transpose((2, 0, 1)).copy()  # chw, RGB order,[0,255]\n","            img_tensor = torch.from_numpy(img_tensor).float().div(255)  # chw , FloatTensor type,[0,1]\n","            img_tensor = img_tensor.unsqueeze(0)  # nch*w\n","            x = img_tensor.to(device)\n","            model.eval()\n","            out = model(x, version=\"b\")\n","            # convert the pytorch output into cv2\n","            out = out.data.cpu().squeeze().numpy()\n","            out = np.transpose(out, (1, 2, 0))\n","            out = cv2.cvtColor(out, cv2.COLOR_RGB2BGR)\n","            out = img_as_ubyte(out)\n","            video_tracked.write(out)\n","\n","        except Exception as e:\n","            print(e)\n","    else:\n","        break"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}